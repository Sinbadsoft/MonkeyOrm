{"note":"Don't delete this file! It's used internally to help with page regeneration.","google":"UA-32892181-1","tagline":"A small and powerful ORM that doesn't get in your way.","name":"MonkeyOrm","body":"## Save anything\r\nPOCOs:\r\n\r\n```csharp\r\nconnection.Save(\"Users\", new User { Name = \"Anne\", Age = 31 });\r\n```\r\nAnonymous:\r\n\r\n```csharp\r\nconnection.Save(\"Users\", new { Name = \"Jhon\", Age = 26 });\r\n```\r\nHashes: Dictionary<>, Dictionary, ExpandoObject or NameValueCollection\r\n\r\n```csharp\r\nconnection.Save(\"Users\", new Dictionary<string, object>\r\n                                {\r\n                                    { \"Name\", \"Fred\" },\r\n                                    { \"Age\", 22 }\r\n                                });\r\n```\r\nGet back the auto generated serial id if any:\r\n\r\n```csharp\r\nint pabloId;\r\nconnection.Save(\"Users\", new User { Name = \"Pablo\", Age = 49 }, out pabloId);\r\n```\r\n\r\n##### What the heck is `connection`?\r\nIn order to make things easier for the client code, the above extension methods are defined for a bunch of types. Actually, `connection` can be a: [IDbConnection](http://msdn.microsoft.com/en-us/library/system.data.idbconnection.aspx), [IDbTransaction](http://msdn.microsoft.com/en-us/library/system.data.idbtransaction.aspx), \r\nany function acting as a connection factory Func<IDbConnection> or the MonkeyOrm defined interface [IConnectionFactory](https://github.com/Sinbadsoft/MonkeyOrm/blob/master/MonkeyOrm/IConnectionFactory.cs).\r\n\r\n\r\n## Read just one item\r\nReads only the first element, if any, from the result set.\r\n\r\n```csharp\r\nvar joe = connection.ReadOne(\"Select * From Users Where @Id = id\", new { id = 1 });\r\n```\r\nYou can also read computed data\r\n\r\n```csharp\r\nvar stats = connection.ReadOne(\"Select Max(Age) As Max, Min(Age) As Min From Users\");\r\nConsole.WriteLine(\"Max {0} - Min {1}\", stats.Max, stats.Min);\r\n```\r\n\r\n## Read'em All\r\nBulk fetches the whole result set in memory as a list.\r\n\r\n```csharp\r\nvar users = connection.ReadAll(\"Select * From Users Where Age > @age\", new { age = 30 });\r\n```\r\n\r\n## Stream Reading\r\nWraps query results in an enumerable. Items are then lazily loaded on enumeration. Here is an example how records (thousands or millions) can be streamed to a file:\r\n```csharp\r\nvar userStream = connection.ReadStream(\"Select * From Users\");\r\n\r\nusing(var fileStream = new StreamWriter(Path.GetTempFileName()))\r\nforeach (var user in userStream)\r\n{\r\n    fileStream.WriteLine(\"{0} - {1}\", user.Name, user.Age);\r\n}\r\n```\r\n\r\n## Update\r\n\r\n```csharp\r\nconnection.Update(\"Users\", new { CanBuyAlchohol = true }, \"Age >= @age\", new { age = 21 });\r\n```\r\n\r\n## Save or Update\r\naka Upsert. Attempts to save the data. If a record with the same key is already present, it is updated.\r\n```csharp\r\nconnection.SaveOrUpdate(\"Users\", new User { Name = \"Anne\", Age = 32 });\r\n```\r\n\r\n## Delete\r\n```csharp\r\nconnection.Delete(\"Users\", \"Name=@name\", new { name = \"Sauron\" });\r\n```\r\n\r\n## Transactions\r\n```csharp\r\nint spockId = connection.InTransaction(autocommit: true).Do(t =>\r\n{\r\n    int id;\r\n    t.Save(\"Users\", new { Name = \"Spock\", Age = 55 }, out id);\r\n    t.Save(\"Profiles\", new { UserId = id, Bio = \"Federation Ambassador\" });\r\n    return id;\r\n});\r\n```\r\nThe transaction block return a value of any type or return void. The transaction can be manually committed at any point by invoking `t.Commit()`. Setting `autocommit` to `true` will insert a call to `Commit()` at the end of the block execution.\r\n\r\nTransaction isolation level can be controlled through the `isolation` parameter:\r\n```csharp\r\nconnection.InTransaction(true, IsolationLevel.Serializable).Do(t =>\r\n{\r\n    var james = t.ReadOne(\"Select * From Users Where Name=@name\", new { name = \"James\" });\r\n    t.Update(\"Users\", new { Age = james.Age + 15 }, \"Id=@Id\", new { james.Id });\r\n});\r\n```\r\n## Batch insertion\r\nBatch insertion enables insertion of enumerable data sets; whether this data set is held in memory or streamed from any other source (file, database, network etc.).\r\n\r\n```csharp\r\nconnection.SaveBatch(\"Users\", new[]\r\n    {\r\n        new User { Name = \"Monica\", Age = 34 },\r\n        new User { Name = \"Fred\", Age = 58 },\r\n        // ...\r\n    });\r\n```\r\n\r\nBy default, one object at a time is read from the provided set and inserted in the database. In order to tune performance/bandwidth more elements can be loaded and inserted at once through the `chunkSize` parameter.\r\n\r\nIn the following snippet, 100 objects are loaded and inserted at a time from the provided enumerable.\r\n```csharp\r\nconnection.SaveBatch(\"Users\", LoadDataFromRemoteSource(), 100);\r\n```\r\n\r\nBatch insertion can also be wrapped in a transaction\r\n```csharp\r\nconnection.InTransaction().SaveBatch(\"Users\", users);\r\n```\r\n\r\n# Features Summary\r\n* CRUD with No code annotations; No config files and No base classes to inherit from.\r\n* Transactions.\r\n* Object slicing on insertion with white or black lists.\r\n* Bulk-fetching: fetches query results and loads them in memory.\r\n* Data streaming: wraps query results in an enumerable. Items are lazily loaded on enumeration.\r\n* Batch insertion: streams any enumerable into the db. Fine-grained control on the number of inserted objects per query in order to control performance and bandwidth.\r\n* Blobbing: accepts interceptor callbacks to handle object serialization.\r\n* No fancy DSL, only SQL\r\n* Stateless, no caching\r\n\r\n# Installation\r\n```powershell\r\nInstall-Package MonkeyOrm.MySql\r\n```"}
